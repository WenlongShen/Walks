{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic: Machine Learning from Disaster\n",
    "\n",
    "### Overview\n",
    "\n",
    "Titanic可谓是Kaggler的必经之路。我们以其为例，走一个完整的机器学习分析流程。\n",
    "\n",
    "### Step 1: 问题分析\n",
    "\n",
    "关于Titanic的相关描述可参考官网，这是一个二分类的基本问题。\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
    "\n",
    "### Step 2: 数据整理\n",
    "\n",
    "首先加载必要的库。数据这块儿用pandas，模型用scikit-learn和xgboost，计算的库numpy和scipy，作图用matplotlib和seaborn等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: UTF-8 -*- \n",
    "#!/usr/bin/env python\n",
    "\n",
    "# system parameters\n",
    "import sys\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "# functions for data processing and analysis\n",
    "import pandas as pd\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "# machine learning algorithms\n",
    "import sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process, neural_network\n",
    "from sklearn import feature_selection, model_selection, metrics\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "print(\"xgboost version: {}\". format(xgboost.__version__))\n",
    "\n",
    "# scientific computing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "# data visualization\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "from IPython import display\n",
    "print(\"matplotlib version: {}\". format(mpl.__version__))\n",
    "print(\"seaborn version: {}\". format(sns.__version__))\n",
    "print(\"IPython version: {}\". format(IPython.__version__))\n",
    "# Configure Visualization Defaults\n",
    "# %matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use(\"ggplot\")\n",
    "sns.set_style(\"white\")\n",
    "pylab.rcParams[\"figure.figsize\"] = 12,8\n",
    "\n",
    "# misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('-'*25)\n",
    "# Input data files are available in the \"datasets/titanic/\" directory.\n",
    "# Listing all files\n",
    "from subprocess import check_output\n",
    "data_dir = \"datasets/titanic/\"\n",
    "print(check_output([\"ls\", data_dir]).decode(\"utf8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "接下来我们看看数据的基本情况，可以使用info、describe等函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir+\"train.csv\")\n",
    "test = pd.read_csv(data_dir+\"test.csv\")\n",
    "train.info()\n",
    "#test.info()\n",
    "#train.describe(include = \"all\")\n",
    "#data_train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人习惯将train和test合并做数据整理和特征工程，另外PassengerId仅仅用于数据标识，Ticket意义不太明确，我们将这两项去掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving passenger id in advance in order to submit later.\n",
    "passengerId = test.PassengerId\n",
    "\n",
    "train_len = len(train)\n",
    "dataset =  pd.concat([train, test], axis=0).reset_index(drop=True)\n",
    "#dataset.info()\n",
    "#dataset.head()\n",
    "drop_column = [\"PassengerId\", \"Ticket\"]\n",
    "dataset.drop(drop_column, axis=1, inplace = True)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们统计一下原数据中的缺失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Data columns with null values:\\n\", dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理缺失值是数据预处理重要的一环，往往要综合考虑，多次尝试。常见的方法除均值、中值、众值外，还包括关联性考察、重新编码、去除缺失值过多的特征项、利用机器学习赋值等。\n",
    "\n",
    "这里我们先看看Embarked特征项的两个缺失值，考察下Embarked特征项的整体情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[dataset.Embarked.isnull()]\n",
    "pd.DataFrame(dataset.Embarked.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，出现频率最高的为S，所以一种处理方式就是将缺失值设为众数S。不过，仔细看下Embarked和Pclass/Fare之间的关系会发现，Pclass=1，Fare=80时，Embarked最有可能是C，所以这里我们也可将缺失值设为C。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后续数据中，我们暂时用类似处理Embarked的方法填补Fare，用中值填补Age，用N填补Cabin。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.Embarked.fillna(\"C\", inplace=True)\n",
    "missing_value = dataset[(dataset.Pclass == 3) & (dataset.Embarked == \"S\") & (dataset.Sex == \"male\")].Fare.mean()\n",
    "dataset.Fare.fillna(missing_value, inplace=True)\n",
    "dataset.Age.fillna(dataset.Age.median(), inplace=True)\n",
    "dataset.Cabin.fillna(\"N\", inplace=True)\n",
    "dataset.info()\n",
    "#dataset.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们简单看看train中各特征值之间的关系，包括与目标值Survived之间的关系等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_plot = dataset[:train_len]\n",
    "#histogram comparison of sex, class, and age by survival\n",
    "h = sns.FacetGrid(data_plot, row = \"Sex\", col = \"Pclass\", hue = \"Survived\")\n",
    "h.map(plt.hist, \"Age\", alpha = .75)\n",
    "h.add_legend()\n",
    "#pair plots of entire dataset\n",
    "pp = sns.pairplot(data_plot, hue = \"Survived\", palette = \"deep\", size=1.2, diag_kind = \"kde\", diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n",
    "pp.set(xticklabels=[])\n",
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={\"shrink\":.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor=\"white\",\n",
    "        annot_kws={\"fontsize\":12 }\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Pearson Correlation of Features\", y=1.05, size=15)\n",
    "correlation_heatmap(data_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3: 特征工程\n",
    "\n",
    "特征工程可谓是个见仁见智见能力的事情。\n",
    "\n",
    "我们逐个特征项来做吧，首先是Age。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.kdeplot(data_plot.Age[data_plot.Survived == 0], color=\"Red\", shade = True)\n",
    "g = sns.kdeplot(data_plot.Age[data_plot.Survived == 1], ax =g, color=\"Blue\", shade= True)\n",
    "g.set_xlabel(\"Age\")\n",
    "g.set_ylabel(\"Frequency\")\n",
    "g = g.legend([\"Not Survived\",\"Survived\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到整体差异不大，但儿童的生存率出现峰值，是人性的光辉吧。我们不妨抽提儿童作为新的特征，整体还可以按年龄分组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"AgeBin\"] = pd.cut(dataset.Age.astype(int), 5)\n",
    "dataset[\"IsChild\"] = [1 if i<16 else 0 for i in dataset.Age]\n",
    "#dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin项描述了客舱编号，其开头字母可能具有信息，我们不妨抽提。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset.Cabin.head()\n",
    "dataset.Cabin = [i[0] for i in dataset.Cabin]\n",
    "g = sns.factorplot(y=\"Survived\", x=\"Cabin\", data=dataset, kind=\"bar\")\n",
    "g = g.set_ylabels(\"Survival Probability\")\n",
    "#dataset = pd.get_dummies(dataset, columns = [\"Cabin\"], prefix=\"Cabin\")\n",
    "#dataset.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Fare，我们查看下分布，并使用不同于Age的分组方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.kdeplot(data_plot.Fare[data_plot.Survived == 0], color=\"Red\", shade = True)\n",
    "g = sns.kdeplot(data_plot.Fare[data_plot.Survived == 1], ax =g, color=\"Blue\", shade= True)\n",
    "g.set_xlabel(\"Fare\")\n",
    "g.set_ylabel(\"Frequency\")\n",
    "g = g.legend([\"Not Survived\",\"Survived\"])\n",
    "dataset[\"FareBin\"] = pd.qcut(dataset.Fare, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Name，我们参考论坛的一些帖子，抽提Title作为特征项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Title\"] = dataset.Name.str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n",
    "title_names = (dataset.Title.value_counts() < 5)\n",
    "dataset.Title = dataset.Title.apply(lambda x: \"Misc\" if title_names.loc[x] == True else x)\n",
    "#dataset.Title.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它的特征项，我们也相应处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"FamilySize\"] = dataset.SibSp + dataset.Parch + 1\n",
    "dataset[\"IsAlone\"] = 1\n",
    "dataset[\"IsAlone\"].loc[dataset.FamilySize > 1] = 0\n",
    "dataset[\"IsMother\"] = np.where((dataset.Age > 18) & \n",
    "                               (dataset.Title != 'Miss') & \n",
    "                               (dataset.Parch > 0) & \n",
    "                               (dataset.Sex == 'female'), \n",
    "                               1, 0)\n",
    "#dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为方便处理，我们需要进行一些编码、转换等工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = LabelEncoder()\n",
    "dataset[\"Cabin_Code\"] = label.fit_transform(dataset.Cabin)\n",
    "dataset[\"Sex_Code\"] = label.fit_transform(dataset.Sex)\n",
    "dataset[\"Embarked_Code\"] = label.fit_transform(dataset.Embarked)\n",
    "dataset[\"Title_Code\"] = label.fit_transform(dataset.Title)\n",
    "dataset[\"AgeBin_Code\"] = label.fit_transform(dataset.AgeBin)\n",
    "dataset[\"FareBin_Code\"] = label.fit_transform(dataset.FareBin)\n",
    "drop_column = [\"Cabin\", \"Sex\", \"Embarked\", \"Title\", \"AgeBin\", \"FareBin\", \"Name\"]\n",
    "dataset.drop(drop_column, axis=1, inplace = True)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: 模型建立\n",
    "\n",
    "先将train和test分开，并设好cv。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = dataset[:train_len]\n",
    "test_x = dataset[train_len:]\n",
    "train_y = pd.DataFrame(train_x.Survived)\n",
    "train_x.drop(\"Survived\", axis=1, inplace = True)\n",
    "test_x.drop(\"Survived\", axis=1, inplace = True)\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_x = ss.fit_transform(train_x)\n",
    "test_x = ss.transform(test_x)\n",
    "\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n",
    "#train_x.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们一次性尝试多个模型的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Machine Learning Algorithm (MLA) Selection and Initialization\n",
    "MLA = [\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "    \n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    #linear_model.Perceptron(),\n",
    "    \n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    #naive_bayes.MultinomialNB(),\n",
    "    #naive_bayes.ComplementNB(),\n",
    "    \n",
    "    #Nearest Neighbor\n",
    "    neighbors.KNeighborsClassifier(),\n",
    "    #neighbors.RadiusNeighborsClassifier(radius=50),\n",
    "    \n",
    "    #NN\n",
    "    neural_network.MLPClassifier(),\n",
    "    \n",
    "    #SVM\n",
    "    svm.SVC(probability=True),\n",
    "    svm.NuSVC(probability=True),\n",
    "    #svm.LinearSVC(),\n",
    "    \n",
    "    #Trees    \n",
    "    #tree.DecisionTreeClassifier(),\n",
    "    #tree.ExtraTreeClassifier(),\n",
    "    \n",
    "    #Discriminant Analysis\n",
    "    #discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    XGBClassifier()    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面评价一下各个模型的优劣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLA_columns = [\"MLA Name\",\"MLA Parameters\",\"MLA Train Accuracy Mean\",\"MLA Test Accuracy Mean\",\"MLA Test Accuracy 3*STD\",\"MLA Time\"]\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "MLA_predict = train_y.copy()\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    MLA_compare.loc[row_index, \"MLA Name\"] = MLA_name\n",
    "    MLA_compare.loc[row_index, \"MLA Parameters\"] = str(alg.get_params())\n",
    "    \n",
    "    cv_results = model_selection.cross_validate(alg, train_x, train_y, cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, \"MLA Time\"] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, \"MLA Train Accuracy Mean\"] = cv_results[\"train_score\"].mean()\n",
    "    MLA_compare.loc[row_index, \"MLA Test Accuracy Mean\"] = cv_results[\"test_score\"].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, \"MLA Test Accuracy 3*STD\"] = cv_results[\"test_score\"].std()*3   #let's know the worst that can happen!\n",
    "    \n",
    "    #save MLA predictions\n",
    "    alg.fit(train_x, train_y)\n",
    "    MLA_predict[MLA_name] = alg.predict(train_x)\n",
    "    \n",
    "    row_index+=1\n",
    "    \n",
    "MLA_compare.sort_values(by = [\"MLA Test Accuracy Mean\"], ascending = False, inplace = True)\n",
    "MLA_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以看看各模型预测结果间的相关性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_heatmap(MLA_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: 模型优化\n",
    "\n",
    "常见的优化方法主要有调参、特征选择等。这里我们尝试使用GridSearchCV进行调参。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vote_est = [\n",
    "    ('ada', ensemble.AdaBoostClassifier()),\n",
    "    ('bc',  ensemble.BaggingClassifier()),\n",
    "    ('etc', ensemble.ExtraTreesClassifier()),\n",
    "    ('gbc', ensemble.GradientBoostingClassifier()),\n",
    "    ('rfc', ensemble.RandomForestClassifier()),\n",
    "    ('gpc', gaussian_process.GaussianProcessClassifier()),\n",
    "    ('lr',  linear_model.LogisticRegressionCV()),\n",
    "    ('sgd', linear_model.SGDClassifier()),\n",
    "    #('lrp', linear_model.Perceptron()),\n",
    "    ('bnb', naive_bayes.BernoulliNB()),\n",
    "    ('gnb', naive_bayes.GaussianNB()),\n",
    "    ('knn', neighbors.KNeighborsClassifier()),\n",
    "    ('mlp', neural_network.MLPClassifier()),\n",
    "    ('svc', svm.SVC(probability=True)),\n",
    "    ('nu',  svm.NuSVC(probability=True)),\n",
    "    #('lsv', svm.LinearSVC()),\n",
    "    ('qd', discriminant_analysis.QuadraticDiscriminantAnalysis()),\n",
    "    ('xgb', XGBClassifier())\n",
    "]\n",
    "\n",
    "grid_seed = [5362]\n",
    "\n",
    "grid_param = [\n",
    "            # AdaBoostClassifier\n",
    "            [{\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [.05, .1, .2],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "       \n",
    "            # BaggingClassifier\n",
    "            [{\n",
    "            'n_estimators': [200, 300, 400],\n",
    "            'max_samples': [.2, .3, .5],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "            \n",
    "            # ExtraTreesClassifier\n",
    "            [{\n",
    "            'n_estimators': [50, 100, 300],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [6, 8, None],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "\n",
    "            # GradientBoostingClassifier\n",
    "            [{\n",
    "            'learning_rate': [.1, .3], \n",
    "            'n_estimators': [100, 300, 500], \n",
    "            'max_depth': [2, 4, 6],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "\n",
    "            # RandomForestClassifier\n",
    "            [{\n",
    "            'n_estimators': [100, 300, 500],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [6, 8, None],\n",
    "            'oob_score': [True, False],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "    \n",
    "            # GaussianProcessClassifier\n",
    "            [{    \n",
    "            'max_iter_predict': [50, 100, 300],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "    \n",
    "            # LogisticRegressionCV\n",
    "            [{\n",
    "            'fit_intercept': [True, False],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "    \n",
    "            # SGDClassifier\n",
    "            [{\n",
    "            'fit_intercept': [True, False],\n",
    "            'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge'],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "    \n",
    "            # Perceptron\n",
    "            #[{\n",
    "            #'fit_intercept': [True, False],\n",
    "            #'random_state': grid_seed\n",
    "            #}],\n",
    "            \n",
    "            # BernoulliNB\n",
    "            [{\n",
    "            'alpha': [.5, .75, 1.0],\n",
    "            }],\n",
    "    \n",
    "            # GaussianNB\n",
    "            [{}],\n",
    "    \n",
    "            # KNeighborsClassifier\n",
    "            [{\n",
    "            'n_neighbors': [5,6,7,8],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'metric': ['minkowski'],\n",
    "            'leaf_size': [16, 22, 26, 30]\n",
    "            }],\n",
    "    \n",
    "            # MLPClassifier\n",
    "            [{\n",
    "            'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "            'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "            \n",
    "            # SVC\n",
    "            [{\n",
    "            'C': [3,4,5,6],\n",
    "            'gamma': [.1, .25, .5],\n",
    "            'decision_function_shape': ['ovo', 'ovr'],\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "    \n",
    "            # NuSVC\n",
    "            [{\n",
    "            'gamma': [.1, .25, .5],\n",
    "            'decision_function_shape': ['ovo', 'ovr'],\n",
    "            'probability': [True],\n",
    "            'random_state': grid_seed\n",
    "            }],\n",
    "    \n",
    "            # LinearSVC\n",
    "            #[{\n",
    "            #'C': [1, 2],\n",
    "            #'random_state': grid_seed\n",
    "            #}],\n",
    "    \n",
    "            # QuadraticDiscriminantAnalysis\n",
    "            [{}],\n",
    "    \n",
    "            #XGBClassifier\n",
    "            [{\n",
    "            'learning_rate': [.02, .03, .05], #default: .3\n",
    "            'max_depth': [8, 10, 12], #default 2\n",
    "            'n_estimators': [300, 400], \n",
    "            'colsample_bytree': [0.6, 0.8],\n",
    "            'colsample_bylevel': [0.6, 0.8],\n",
    "            'seed': grid_seed  \n",
    "            }]   \n",
    "        ]\n",
    "\n",
    "start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n",
    "for clf, param in zip (vote_est, grid_param):\n",
    "    start = time.perf_counter()        \n",
    "    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = \"roc_auc\")\n",
    "    best_search.fit(train_x, train_y)\n",
    "    run = time.perf_counter() - start\n",
    "\n",
    "    best_param = best_search.best_params_\n",
    "    print(\"The best parameter for {} is {} with a runtime of {:.2f} seconds.\".format(clf[1].__class__.__name__, best_param, run))\n",
    "    clf[1].set_params(**best_param) \n",
    "\n",
    "\n",
    "run_total = time.perf_counter() - start_total\n",
    "print(\"Total optimization time was {:.2f} minutes.\".format(run_total/60))\n",
    "\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "面对一众模型，我们在提交最终预测结果的时候还可以选择使用ensemble，这里以voting为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensembleVote = ensemble.VotingClassifier(estimators = vote_est , voting = \"hard\")\n",
    "ensembleVote.fit(train_x, train_y)\n",
    "prediction = ensembleVote.predict(test_x).astype(int)\n",
    "submission = pd.DataFrame(data={\n",
    "    \"PassengerId\": passengerId,\n",
    "    \"Survived\": prediction\n",
    "})\n",
    "#submission.info()\n",
    "submission.to_csv(data_dir+\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提交到Kaggle，最终得分0.78947。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
